{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL for downloading the videos\n",
    "video_url = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "The function getImages() is used to download a single .ts file or multiple .ts files in parallel and extracts the first frame as a .jpg image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function accepts name of a video file, downloads it and extracts the first frame as a .jpg image\n",
    "def extractImageFromVideo(file_name):\n",
    "    try:\n",
    "        print('downloading {}{}.ts...'.format(video_url,file_name))\n",
    "        video = cv2.VideoCapture(video_url+file_name+'.ts')\n",
    "        if(video.isOpened() == False):\n",
    "            video.open(video_url+file_name+'.ts')  #Initialize the video if it did not open properly\n",
    "        print('extracting image... ', end='')\n",
    "        flag, frame = video.read()\n",
    "        if(flag):        #Checking if the frame was read correctly or not\n",
    "            cv2.imwrite(file_name+'.jpg',frame)\n",
    "            print('wrote {}.jpg'.format(file_name))\n",
    "            video.release()\n",
    "            return True\n",
    "        else:\n",
    "            print('Error in reading the video file')\n",
    "            video.release()\n",
    "    except Exception as e:\n",
    "        print('There is no video file named {}.ts'.format(file_name))\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function downloads a single .ts file (concurrency=False) or multiple .ts files (concurrency=True) in parallel\n",
    "def getImages(file_names, concurrency=False):\n",
    "    if(concurrency):\n",
    "        p = Pool()     \n",
    "        return(p.map(extractImageFromVideo, file_names))  #file_names will be a list of video file names\n",
    "    else:\n",
    "        return(extractImageFromVideo(file_names))   #file_names will be the name of a single video file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "The function detectCar() accepts file name of an image, runs the YOLO network and detects if there is a car or not at the specified parking spot. For doing this, I'm taking two reference points (similar to anchor boxes in YOLO) within the parking spot- one for the top left corner and other for the bottom right corner and then calculating L2 distance between the reference points and all the bounding boxes predicted by the YOLO network. Finally, if any one of these distances is less than some threshold value, I am predicting that there is a car at the parking spot. <b>For more details, please refer to the readme file. </b> </br>\n",
    "<img src=\"./car_detection.png\" width=\"500px\" height=\"500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from darkflow.net.build import TFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options to be specified for running the yolo network\n",
    "option = {\n",
    "    'model' : 'cfg/yolo.cfg',\n",
    "    'load' : 'weights/yolo.weights',\n",
    "    'threshold' : 0,\n",
    "    'gpu' : 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(x1, y1, x2, y2):\n",
    "    return (int(sqrt(pow(x1-x2,2)+pow(y1-y2,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference point for the top left corner and the bottom right corner are (214,219) and (241,231) respectively, with threshold value equal to 80 \n",
    "def detectCar(file_name):\n",
    "    network = TFNet(option)\n",
    "    image = cv2.imread(file_name)\n",
    "    print('running yolo on {}...'.format(file_name))\n",
    "    result = network.return_predict(image)\n",
    "    for i in range(len(result)):    #Looping through all the predictions\n",
    "        info = result[i]\n",
    "        if(info['label'] == 'car'):  \n",
    "            topX = int(info['topleft']['x'])\n",
    "            topY = int(info['topleft']['y'])\n",
    "            bottomX = int(info['bottomright']['x'])\n",
    "            bottomY = int(info['bottomright']['y'])\n",
    "            l2_dist = L2(214,219,topX,topY) + L2(241,231,bottomX,bottomY)\n",
    "            if(l2_dist<=80):\n",
    "                print('car detected!')\n",
    "                return\n",
    "    print('no car detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "The function detectCarAtSpot() can be used to analyze any parking spot of your choice. It works similar to the detectCar() function except that it has reference points and threshold values for every parking spot. Over here, I have numbered parking spots (leaving ones at the extreme ends as they were often not being detected by the YOLO network) from 0-7 starting from the left-hand side of the image. Hence, the parking spot specified in this assignment gets the number 4. <b>For more details, please refer to the readme file. </b> </br>\n",
    "<img src=\"./reference_points.jpg\" width=\"500px\" height=\"500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold value for all the parking spots\n",
    "parking_thresholds = [60,60,60,60,80,72,70,45]\n",
    "#Reference points (topX,topY,bottomX,bottomY) for all the parking spots\n",
    "parking_spots = [(25,242,37,249),(56,236,75,245),(98,233,121,245),(145,225,173,236),(214,219,241,231),(637,209,680,231),(707,210,732,232),(752,214,782,240)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the file name of an image along with a parking spot number and detects if a car is present or not\n",
    "def detectCarAtSpot(file_name, spot_no): \n",
    "    network = TFNet(option)\n",
    "    image = cv2.imread(file_name)\n",
    "    print('running yolo on {}...'.format(file_name))\n",
    "    result = network.return_predict(image)\n",
    "    for i in range(len(result)):    #Looping through all the predictions\n",
    "        info = result[i]\n",
    "        if(info['label'] == 'car'):\n",
    "            topX = int(info['topleft']['x'])\n",
    "            topY = int(info['topleft']['y'])\n",
    "            bottomX = int(info['bottomright']['x'])\n",
    "            bottomY = int(info['bottomright']['y'])\n",
    "            l2_dist = L2(parking_spots[spot_no][0],parking_spots[spot_no][1],topX,topY) + L2(parking_spots[spot_no][2],parking_spots[spot_no][3],bottomX,bottomY)\n",
    "            if(l2_dist<=parking_thresholds[spot_no]):\n",
    "                print('car detected!')\n",
    "                return\n",
    "    print('no car detected!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "The function compareImages() accepts file name of two images and checks if both the images is of the same car or not. To do this, I’m first analyzing the number of cars that were parked within the given time range and if it is greater than one (meaning the car is not there anymore but can come again in future) then I am going for template matching because we know that those cars will be rotation and scale invariant and template matching works well when these conditions hold true. So, I’m cropping out the area containing the parking spot from the images, converting the cropped image into gray scale, normalizing and computing L2 distance between them (pixel wise). Finally, if this distance is less than some threshold value then I am predicting that both of these images are of the same car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will check if there was only one car present at the parking spot within the given time interval \n",
    "def sameCarHelper(from_time, to_time):\n",
    "    cur_time = from_time\n",
    "    while(cur_time<=to_time):\n",
    "        check = getImages(str(cur_time))\n",
    "        if(check):   #checking if a video exists with the given file_name or not\n",
    "            detected = detectCar(str(cur_time)+'.jpg')\n",
    "            if(detected==False): #if there is no car at the current time step means the car at time1 has left but it can come back later after some time\n",
    "                return False\n",
    "            cur_time = cur_time + 4\n",
    "        else:\n",
    "            cur_time = cur_time + 1  #increasing time by one if no video was found with the current time step\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeImage(image):\n",
    "    return ((image - image.min())/ (image.max() - image.min()))  #Normalizing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the file name of two images and checks if they belong to the same car or not\n",
    "def compareImages(file1_name, file2_name):\n",
    "    time1 = int(file1_name[:file1_name.index('.jpg')])\n",
    "    time2 = int(file2_name[:file2_name.index('.jpg')])\n",
    "    check = False\n",
    "    if(time1<=time2):\n",
    "        check = sameCarHelper(time1, time2)\n",
    "    else:\n",
    "        check = sameCarHelper(time2, time1)\n",
    "    print('comparing {} and {}... '.format(file1_name,file2_name), end='')\n",
    "    if(check):\n",
    "        print('same car!')\n",
    "        return\n",
    "    #If sameCarHelper returns false then we have to check if the car comes back again later\n",
    "    image1 = cv2.imread(file1_name)\n",
    "    image2 = cv2.imread(file2_name)\n",
    "    cropped_image1 = image1[189:252,185:274]\n",
    "    cropped_image2 = image2[189:252,185:274]\n",
    "    gray_image1 = cv2.cvtColor(cropped_image1,cv2.COLOR_BGR2GRAY)\n",
    "    gray_image2 = cv2.cvtColor(cropped_image2,cv2.COLOR_BGR2GRAY)\n",
    "    normalized_image1 = normalizeImage(gray_image1)\n",
    "    normalized_image2 = normalizeImage(gray_image2)\n",
    "    l2_norm = np.sqrt(np.sum(np.power(normalized_image1-normalized_image2,2)))\n",
    "    if(l2_norm<=10):\n",
    "        print('same car!')\n",
    "    else:\n",
    "        print('not same car!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "The function analyzeCars() accepts a time range and outputs each car that was detected and how long it was parked for (approximately). To do this, I'm keeping the information of any car that was parked in the previous time step and if a car was detected or not at the current time step. With this knowledge, I'm predicting cars that were detected and for how long were they parked. <b>For more details, please refer to the readme file.</b>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeCars(from_time, to_time):\n",
    "    if(to_time-from_time < 0):\n",
    "        print('Invalid time interval provided')\n",
    "        return\n",
    "    if(os.path.isdir('./output') == False):    #Checking if output folder existst or not\n",
    "        os.mkdir('output')\n",
    "    car_found = False  #this is used to check if a car was detected in the previous time steps\n",
    "    parked_at = 0   #this is used to store the starting time of a parked car\n",
    "    car = None  #this is used to store the frame containing a car when it will be found for the first time at the parking spot\n",
    "    print('analyzing every car from {} to {}'.format(from_time,to_time))\n",
    "    cur_time = from_time\n",
    "    valid_endTime = from_time     #This will hold the correct end time for the last car (if to_time>max time in the database)\n",
    "    while(cur_time<=to_time):\n",
    "        check = getImages(str(cur_time))\n",
    "        if(check):   #checking if a video exists with the given file_name or not\n",
    "            detected = detectCar(str(cur_time)+'.jpg')\n",
    "            if(detected==False and car_found): #if currently there is no car but car_found is true, means that the previously parked car has left\n",
    "                car_found = False\n",
    "                parking_time = cur_time-parked_at\n",
    "                seconds = parking_time%60\n",
    "                minutes = int((parking_time-seconds)/60)\n",
    "                print('found car at {}. parked until {} ({} minutes and {} seconds).'.format(parked_at,cur_time,minutes,seconds))\n",
    "                cv2.imwrite('output/{}-{}min{}sec.jpg'.format(parked_at,minutes,seconds),car)\n",
    "                print('... wrote output/{}-{}min{}sec.jpg'.format(parked_at,minutes,seconds))\n",
    "                parked_at = 0\n",
    "            elif(detected and car_found == False): #if a car is detected and car_found is false, means that the car is detected for the first time\n",
    "                car_found = True\n",
    "                parked_at = cur_time\n",
    "                car = cv2.imread(str(cur_time)+'.jpg')\n",
    "            valid_endTime = cur_time\n",
    "            cur_time = cur_time + 4\n",
    "        else:\n",
    "            cur_time = cur_time + 1  #increasing time by one if no video was found with the current time step\n",
    "    if(parked_at != 0):  #checking if there was still a car parked at the end of the interval\n",
    "        parking_time = valid_endTime-parked_at\n",
    "        seconds = parking_time%60\n",
    "        minutes = int((parking_time-seconds)/60)\n",
    "        print('found car at {}. parked until {} ({} minutes and {} seconds).'.format(parked_at,valid_endTime,minutes,seconds))\n",
    "        cv2.imwrite('output/{}-{}min{}sec.jpg'.format(parked_at,minutes,seconds),car)\n",
    "        print('... wrote output/{}-{}min{}sec.jpg'.format(parked_at,minutes,seconds))\n",
    "    print('no more cars found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "The function detectColor() accepts file name of an image containing a car and predicts the color of the car. To do this, I'm first converting the image into the HSV color scheme, cropping out a small area containing the car, taking mean of the HSV values over the pixels from the cropped image and then using these average HSV values to predict different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the average HSV values of the cropped image and predicts the color of the car\n",
    "def whichColor(hue, saturation, value):\n",
    "    if(value==0 or (value<=100 and saturation<=100)):\n",
    "        return 'Black'\n",
    "    elif((value>=230 and saturation<140) or (value>=140 and saturation<40)):\n",
    "        return 'White'\n",
    "    elif(saturation<=100):\n",
    "        return 'Grey'\n",
    "    elif(hue>=0 and hue<=4):\n",
    "        return 'Red'\n",
    "    elif(hue>16 and hue<=34):\n",
    "        return 'Yellow'\n",
    "    elif((hue>34 and hue<=90) or (hue>204 and hue<=256) or (hue>288 and hue<=340)):\n",
    "        return 'Green'\n",
    "    elif((hue>90 and hue<=95) or (hue>341 and hue<=348)):\n",
    "        return 'Greenish Blue'\n",
    "    elif((hue>95 and hue<=120) or (hue>349 and hue<=360)):\n",
    "        return 'Blue'\n",
    "    elif((hue>120 and hue<=160)):\n",
    "        return 'Purple'\n",
    "    elif((hue>160 and hue<=184) or (hue>256 and hue<=262)):\n",
    "        return 'Maroon'\n",
    "    elif((hue>4 and hue<=16) or (hue>184 and hue<=204) or (hue>262 and hue<=288)):\n",
    "        return 'Brown'\n",
    "    else:\n",
    "        return 'I cannot guess the color'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function detects color of the car\n",
    "def detectColor(file_name):\n",
    "    image = cv2.imread(file_name)\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    avg_hue = np.mean(hsv_image[189:252,185:274,0])\n",
    "    avg_saturation = np.mean(hsv_image[189:252,185:274,1])\n",
    "    avg_value = np.mean(hsv_image[189:252,185:274,2])\n",
    "    print(whichColor(avg_hue,avg_saturation,avg_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
